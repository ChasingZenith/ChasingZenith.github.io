---
layout: post
title:  "RL"
date:   2018-11-13 09:02:20 +0800
categories: jekyll update
---
---

## 日常记录

### Nov 13

学习机器学习1.1 1.2。

## Reinforcement Learning

Taking some *NOTE*

### 1.1 Reinforcement Learning

> Reinforcement learning is learning what to do-how to **map situations to actions**-so as to maximize a numerical reward signal.

> the basic idea is simply to capture the most important aspects of the real problem facing a learning agent **interacting** over time with its **environment** to achieve a **goal**.

> Reinforcement learning is different from supervised learning, the kind of learning studied in most current research in the field of machine learning. Reinforcement learning is also different from what machine learning researchers call unsupervised learning, which is typically about finding structure hidden in collections of unlabeled data. *Although one might be tempted to think of reinforcement learning as a kind of unsupervised learning because it does not rely on examples of correct behavior, reinforcement learning is trying to maximize a reward signal instead of trying to find hidden structure. Uncovering structure in an agent’s experience can certainly be useful in reinforcement learning, but by itself does not address the reinforcement learning problem of maximizing a reward signal.*

> One of the challenges that arise in reinforcement learning, and not in other kinds of learning, is the **trade-off between exploration and exploitation**.

> Another key feature of reinforcement learning is that it explicitly considers the whole problem of a goal-directed agent **interacting** with an **uncertain environment**.

### 1.2 Examples

> All involve interaction between an active decision-making agent and its environment, within which the agent seeks to achieve a goal despite uncertainty about its environment.

### Chapter 2 Multi-armed Bandits

> In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This **nonassociative** setting is the one in which most prior work involving evaluative feedback has been done,

本章在一个简化的场景中研究强化学习的评价方面(evaluate aspect)。所谓简化的场景，指的是不涉及多个学习的场景(one that does not involve learning to act in more than one situation)，很多之前涉及评价式反馈的研究工作都是在这种非关联(nonassociative)的场景下进行的。

### 2.5 Tracking a Nonstationary Problem

Qestion Why???
> A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:
>
> $$ \sum_{n=1}^{\infty} {\alpha_n(a)} = \infty \,\,\, \text{and} \,\,\,\, \sum_{n=1}^{\infty} {\alpha^2_n(a)} \lt \infty \tag{2.7} $$
>
> The first condition is required to guarantee that the steps are large enough to eventually overcome any initial conditions or random luctuations. The second condition guarantees that eventually the steps become small enough to assure convergence.

$$\sum_{n=1}^{\infty} {\alpha_n(a)} = \infty \,\,\, \text{and} \,\,\,\, \sum_{n=1}^{\infty} {\alpha^2_n(a)} \lt \infty$$


### 2.6 Optimistic Initial Values

Question What is *biased* mean???
> All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, $Q_1 (a)$. In the language of statistics, these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant $\alpha$, the bias is
permanent, though decreasing over time as given by (2.6).

#### Ex 2.6

#### Ex 2.7

I can't prove it by myself

### 2.8 Gradient Bandit Algorithms

Question What is **Gradient ascent** ???
$$ H_{t+1}(a)\dot{=}H_t(a)+\alpha\frac{\partial\Bbb{E}[R_t]}{\partial H_t(a)} $$

$$ \Bbb{E}[R_t]=\sum_x \pi_t(x)q_*(x) $$
